---
title: Logistic regression
tags: Machine-Learning
---

> 로지스틱 회귀는 이진 분류를 위해 사용되는 모델로서, 기존 선형회귀에 로지스틱 함수(sigmoid)를 결합한 형태이다. 관점에 따라 logit 값인 로그 승산(odds)에 대해 선형회귀를 하고 BCELoss를 적용하였다고 볼 수 있다.

## 선형 회귀에서 분류로

선형 회귀는 주어진 데이터를 가장 잘 설명하는 직선을 찾는 문제였다. 여기서 더 나아가 사람들은 클래스가 주어져있을 때 이들을 가장 잘 분리해낼 수 있는 선을 찾아내기를 원했다. 예를 들어, 키에서 몸무게를 뺀 값이 90보다 작아지면 비만이라고 하는 것처럼 매우 간단하게 생각해볼 수 있다. 그러나 모든 사람들을 키와 몸무게로만 비만을 정의내리는 것은 너무 단순한 접근이었다.

![Unit step 함수를 통한 분류](https://github.com/user-attachments/assets/adfd1918-2b56-4881-9272-0dbe3764d563){:.centered}

90에 가까운 사람은 비만이더라도 그 정도가 덜 심하다고 판단할 수 있고, 완벽하게 위 그림처럼 구분되어서 판단할 수 있는 것은 아니다. 따라서 조금 더 부드럽게 그 경계면을 만들어 준 것이 로지스틱 회귀이다. 경계면을 부드럽게 만들어주는 함수를 우리는 **sigmoid function**이라고 말하고 위 그림 처럼 계단처럼 만들어지는 함수를 **step function**이라고 부른다.

![Sigmoid 함수를 통한 분류](https://github.com/user-attachments/assets/5ec40177-dc45-4ff5-9cba-6d067bdfde78){:.centered}

시그모이드 함수를 식으로 나타내면 $1 / (1 + e^{-x})$ 와 같고, 우리는 키와 뭄무게를 잘 조합하여 $x$ 값을 만들어주는 $f$ 를 학습하면 된다. 비만인 사람의 경우 값을 매우 작게 하여 비만에 가깝게 하고, 정상인 사람이라면 값을 크게 만들어 정상쪽에 가깝에 만들어 주면 된다. $f$ 의 시그모이드를 씌운 결과를 $p$ 라고 한다면, 아래의 손실함수를 작성해볼 수 있다.

$$ \mathcal{L} = -\begin{cases} 1 - p & \text{if} & y=0 \\ p & \text{if} & y = 1\end{cases} \qquad\qquad \mathcal{L}=-p^y(1-p)^{(1-y)} $$

여기서 지수 곱셈 연산량을 줄이기 위해 단조 증가함수 중 하나인 로그를 씌워 negative log likelihood loss 를 정의하면 수식이 더 간단해진다.

$$ \mathcal{L} = -y\log p-(1-y)\log (1-p) $$

$p$ 는 로지스틱 회귀에서 학습하고자 한 함수 $f$에 의해 결정된 것이고, 결국 함수에 존재하는 파라미터를 Loss 식에 대해 편미분하여 업데이트를 한다. 이렇게 간단하게나마 로지스틱회귀에 대해서 살펴보았는데, 이것을 다른 방법으로 해석하는 것도 존재한다.

<br>

## Logit 관점의 로지스틱 회귀

로짓(logit)은 보통 sigmoid 함수를 거치기 이전의 값을 말하며 여기서는 로그-승산(odds)를 나타낸다. 결론 부터 말하자면, 로지스틱 회귀는 로그 승산을 선형회귀한 것과 같아진다. 시그모이드 함수를 거친 결과는 확률 $p$ 라고 두었다면 모든 준비는 끝났다. 이제 증명이다.

$$ \frac{1}{1 + e^{-x}} = p $$

$$ 1 + e^{-x} = \frac{1}{p} \quad\Rightarrow\quad e^{-x} = \frac{1 - p}{p} \quad\Rightarrow\quad e^x = \frac{p}{1-p} \quad\Rightarrow\quad x = \ln \left( \frac{p}{1-p} \right) $$

시그모이드 함수의 출력을 입력에 대해서 풀어서 작성하면, 그 식은 로그-승산과 같아지게 되기에 로지스틱 회귀는 로그-승산에 대한 선형회귀라고 해석할 수 있게 된다.